"""
.. module:: PyTorchMalConv
	:synopsis: MalConv CClassifierPytorch wrapper

.. moduleauthor:: Luca Demetrio <luca.demetrio@dibris.unige.it>

"""
import sys
import os
import torchvision.transforms as transforms
import torch
import torch.optim as optim
from torch.autograd import Variable
from torch.utils.data import DataLoader
from secml.array import CArray
from secml.data.c_dataset_pytorch import CDatasetPyTorch
from secml.settings import SECML_PYTORCH_USE_CUDA
from secml.utils.mixed_utils import AverageMeter
from secml.ml.classifiers.loss import CSoftmax
from secml.ml.classifiers.pytorch.c_classifier_pytorch import CClassifierPyTorch
from copy import deepcopy
from secml_malware.models.malconv import MalConv

use_cuda = torch.cuda.is_available() and SECML_PYTORCH_USE_CUDA


class CClassifierMalConvPyTorch(CClassifierPyTorch):
	def grad_tr_params(self, x, y):
		pass

	def grad_loss_params(self, x, y, loss=None):
		pass

	def grad_f_params(self, x, y):
		pass

	def hessian_tr_params(self, x, y):
		pass

	def __init__(
		self,
		learning_rate=1e-2,
		momentum=0.9,
		weight_decay=1e-3,
		epochs=100,
		gamma=0.1,
		lr_schedule=(50, 75),
		batch_size=256,
		regularize_bias=True,
		train_transform=None,
		preprocess=None,
		softmax_outputs=False,
		random_state=None,
		plus_version=False,
		input_shape=(1, 2 ** 20),
		verbose=0,
	):
		super(CClassifierMalConvPyTorch, self).__init__(
			model=MalConv(),
			loss="binary_crossentropy",
			epochs=epochs,
			batch_size=batch_size,
			preprocess=preprocess,
			input_shape=input_shape,
			softmax_outputs=softmax_outputs,
			random_state=random_state,
		)
		self.plus_version = plus_version
		self.verbose = verbose
		self.train_transform = (
			train_transform
			if train_transform is not None
			else transforms.Lambda(lambda p: p.reshape(input_shape[1]))
		)

	@staticmethod
	def _extract_two_dim_vector_from_confidence(output_confidences):
		return torch.Tensor([[1 - p, p] for p in output_confidences])

	def _extract_confidence_from_vector(self, values):
		return values[:, 1].reshape((len(values), 1))

	def predict(self, x, return_decision_function=False, n_jobs=1):
		"""
		Perform classification of each pattern in x.
		If a preprocess has been specified, input is normalized before classification.
		
		Arguments:
			x {CArray} --  Array with new patterns to classify, 2-Dimensional of shape (n_patterns, n_features).

		Keyword Arguments:
			return_decision_function {bool} -- Whether to return the decision_function value along with predictions. Default False.
			n_jobs {int} -- Number of parallel workers to use for classification. Default 1. Cannot be higher than processor's number of cores.

		Returns
			labels {CArray} --  Flat dense array of shape (n_patterns,) with the label assigned to each test pattern. The classification label is the label of the class associated with the highest score.
			scores {CArray} -- Array of shape (n_patterns, n_classes) with classification score of each test pattern with respect to each training class. 
			Will be returned only if `return_decision_function` is True.
		"""
		if not self.is_fitted():
			raise ValueError("make sure the classifier is trained first.")

		x_carray = CArray(x).atleast_2d()

		# Preprocessing data if a preprocess is defined
		if self.preprocess is not None:
			x_carray = self.preprocess.normalize(x_carray)

		x_loader = self._get_test_input_loader(x_carray, n_jobs=n_jobs)

		# Switch to evaluation mode
		self._model.eval()

		scores = None
		for batch_idx, (s, _) in enumerate(x_loader):
			self.logger.info("Classification: {}/{}".format(batch_idx, len(x_loader)))

			if use_cuda is True:
				s = s.cuda()
			s = Variable(s, requires_grad=True)

			with torch.no_grad():
				logits = self._extract_two_dim_vector_from_confidence(self._model(s))
				logits = logits.squeeze(1)
				logits = CArray(logits.data.cpu().numpy()).astype(float)

			if scores is not None:
				scores = scores.append(logits, axis=0)
			else:
				scores = logits

		# The classification label is the label of the class
		# associated with the highest score
		labels = scores.argmax(axis=1).ravel()

		return (labels, scores) if return_decision_function is True else labels

	def _decision_function(self, x, y, n_jobs=1):
		if self.is_clear():
			raise ValueError("make sure the classifier is trained first.")

		x = x.atleast_2d()  # Ensuring input is 2-D

		x_loader = self._get_test_input_loader(x, n_jobs=n_jobs)

		# Switch to evaluation mode
		self._model.eval()

		scores = None
		for batch_idx, (s, _) in enumerate(x_loader):

			# Log progress
			self.logger.info(
				"Classification: {batch}/{size}".format(
					batch=batch_idx, size=len(x_loader)
				)
			)

			if use_cuda is True:
				s = s.cuda()
			s = Variable(s, requires_grad=True)

			with torch.no_grad():
				logits = self._extract_two_dim_vector_from_confidence(self._model(s))
				logits = logits.squeeze(1)
				logits = CArray(logits.data.cpu().numpy()).astype(float)

			# Apply softmax-scaling if needed
			if self.softmax_outputs is True:
				logits = CSoftmax().softmax(logits)

			logits = logits[:, y]  # Extract desired class

			if scores is not None:
				scores = scores.append(logits, axis=0)
			else:
				scores = logits

		return scores.ravel()

	def _apply_constraints(self):
		# remove zeros from weights, allowing only biases to be lower than 0
		self._model.embedding_1.weight.data.clamp_(0)
		self._model.conv1d_1.weight.data.clamp_(0)
		self._model.conv1d_2.weight.data.clamp_(0)
		self._model.dense_1.weight.data.clamp_(0)
		self._model.dense_2.weight.data.clamp_(0)

	def gradient_f_x(self, x, **kwargs):
		"""Returns the gradient of the function on point x.
		
		Arguments:
			x {CArray} -- The point
		
		Raises:
			NotImplementedError: Model do not support gradient
		
		Returns:
			CArray -- the gradient computed on x
		"""
		if self.preprocess is not None:
			# Normalize data before compute the classifier gradient
			x_pre = self.preprocess.normalize(x)
		else:  # Data will not be preprocessed
			x_pre = x
		try:  # Get the derivative of decision_function
			grad_f = self._gradient_f(x_pre, **kwargs)
		except NotImplementedError:
			raise NotImplementedError(
				"{:} does not implement `gradient_f_x`".format(self.__class__.__name__)
			)
		return grad_f

	def _gradient_f(self, x, y=None, w=None, layer=None, sum_embedding=True):
		gradient = self.compute_embedding_gradient(x)
		if sum_embedding:
			gradient = torch.mean(gradient, dim=1)
		if gradient.is_cuda:
			gradient = gradient.cpu()
		return CArray(gradient)

	def load_pretrained_model(
		self, path="secml_malware/data/trained/pretrained_malconv.pth"
	):
		"""Load pretrained model
	
			Keyword Arguments:
				path {str} -- The path of the model, default is internal (default: {"secml_malware/data/trained/pretrained_malconv.pth"})
		"""
		root = os.path.dirname(
			os.path.dirname(os.path.abspath(sys.modules["secml_malware"].__file__))
		)
		self._model.load_simplified_ember_model(os.path.join(root, path))
		self._classes = [0, 1]
		self._n_features = 2 ** 20

	def embed(self, x, transpose=True):
		return self._model.embed(x, transpose=transpose)

	def compute_embedding_gradient(self, x):
		"""Compute the gradient w.r.t. embedding layer
		
		Arguments:
			x {CArray} -- point whenre gradient will be computed
		
		Returns:
			cArray -- the gradient w.r.t. the embedding
		"""
		data = x
		if isinstance(x, CArray):
			data = x.tondarray()
		return self._model.compute_embedding_gradient(data)

	def _get_test_input_loader(self, x, n_jobs=1):
		"""
			Return a loader for input test data.
		"""
		dl = DataLoader(
			CDatasetPyTorch(x),
			batch_size=self.batch_size,
			shuffle=False,
			num_workers=n_jobs - 1,
		)

		# Add a transformation that reshape samples to (C x H x W)
		dl.dataset.transform = transforms.Lambda(
			self.train_transform
			if self.train_transform is not None
			else lambda p: p.reshape(self.input_shape)
		)

		return dl

	def _fit(self, dataset, store_best_params=True, n_jobs=1):
		"""Trains the classifier.
		If specified, train_transform is applied to data.
		Parameters
		----------
		dataset : CDataset
			Training set. Must be a :class:`.CDataset` instance with
			patterns data and corresponding labels.
		store_best_params : bool, optional
			If True (default) the best parameters by classification accuracy
			found during the training process are stored.
			Otherwise, the parameters from the last epoch are stored.
		n_jobs : int, optional
			Number of parallel workers to use for training the classifier.
			Default 1. Cannot be higher than processor's number of cores.
		Returns
		-------
		trained_cls : CClassifier
			Instance of the classifier trained using input dataset.
		Warnings
		--------
		preprocess is not applied to data before training. This behaviour
		will change in the future.
		"""
		if self.start_epoch >= self.epochs:
			self.logger.warning(
				"Maximum number of epochs reached, no training will be performed."
			)
			return self

		# Binarize labels using a OVA scheme
		ova_labels = dataset.get_labels_asbinary()

		# Convert to CDatasetPyTorch and use a dataloader that returns batches
		ds_loader = DataLoader(
			CDatasetPyTorch(dataset.X, ova_labels, transform=self.train_transform),
			batch_size=self.batch_size,
			shuffle=True,
			num_workers=n_jobs - 1,
		)

		# Switch to training mode
		self._model.train()

		# Scheduler to adjust the learning rate depending on epoch
		scheduler = optim.lr_scheduler.MultiStepLR(
			self._optimizer,
			self.lr_schedule,
			gamma=self.gamma,
			last_epoch=self.start_epoch - 1,
		)

		# Storing a copy of the best epoch
		# will be used as the final training state dict
		best_epoch = self.start_epoch
		best_state_dict = deepcopy(self.state_dict())

		for self._start_epoch in range(self.start_epoch, self.epochs):

			scheduler.step()  # Adjust the learning rate
			losses = AverageMeter()  # Logger of the loss value
			acc = AverageMeter()  # Logger of the accuracy

			# Log progress of epoch
			self.logger.info(
				"Epoch: [{curr_epoch}|{epochs}] LR: {lr} - STARTED".format(
					curr_epoch=self.start_epoch + 1,
					epochs=self.epochs,
					lr=scheduler.get_lr()[0],
				)
			)

			for batch_idx, (x, y) in enumerate(ds_loader):
				y = y.squeeze(1)
				y = self._extract_confidence_from_vector(y)
				if use_cuda is True:
					x, y = x.contiguous().cuda(), y.contiguous().cuda()
				x, y = Variable(x, requires_grad=True), Variable(y)

				# Compute output and loss
				logits = self._model(x)
				loss = self.loss(logits, y)

				# compute gradient and do SGD step
				self._optimizer.zero_grad()  # same as self._model.zero_grad()
				loss.backward(torch.ones_like(loss))
				self._optimizer.step()

				if self.plus_version:
					self._apply_constraints()

				size = x.size(0)
				losses.update(loss.item(), size)
				# performance_score = (
				# 	CMetricPyTorchAccuracy()
				# 	.performance_score(
				# 		y_true=self._extract_two_dim_vector_from_confidence(y).data,
				# 		score=self._extract_two_dim_vector_from_confidence(logits).data,
				# 	)[0]
				# 	.item()
				# )
				# acc.update(performance_score, size)

				# Log progress of batch
				self.logger.debug(
					"Epoch: {epoch}, Batch: ({batch}/{size}) "
					"Loss: {loss:.4f} Acc: {acc:.2f}".format(
						epoch=self.start_epoch + 1,
						batch=batch_idx + 1,
						size=len(ds_loader),
						loss=losses.avg,
						acc=acc.avg,
					)
				)

			# Log progress of epoch
			self.logger.info(
				"Epoch: [{curr_epoch}|{epochs}] "
				"Loss: {loss:.4f} Acc: {acc:.2f}".format(
					curr_epoch=self.start_epoch + 1,
					epochs=self.epochs,
					loss=losses.avg,
					acc=acc.avg,
				)
			)

			# Average accuracy after epoch FIXME: ON TRAINING SET
			self._acc = acc.avg

			# If the best parameters should be stored, store the current epoch
			# as best one only if accuracy is higher or at least same
			# (as the loss should be better anyway for latest epoch)
			if store_best_params is True and self.acc < self.best_acc:
				continue  # Otherwise do not store the current epoch
			else:  # Better accuracy or we should store the latest epoch anyway
				self._best_acc = self.acc
				best_epoch = self.start_epoch
				best_state_dict = deepcopy(self.state_dict())

		if store_best_params is True:
			self.logger.info(
				"Best accuracy {:} obtained on epoch {:}".format(
					self.best_acc, best_epoch + 1
				)
			)

		# Restoring the final state to use
		# (could be the best by accuracy score or the latest)
		self.load_state(best_state_dict)

		return self

	def loss(self, x, target):
		"""Return the loss function computed on input.

		Parameters
		----------
		x : torch.Tensor
			Scores as 2D Tensor of shape (N, C).
		target : torch.Tensor
			Targets as 2D Tensor of shape (N, C).

		Returns
		-------
		loss : torch.Tensor
			Value of the loss. Single scalar tensor.

		"""
		x = x.squeeze(1)
		ys = target.squeeze().float()
		output = torch.nn.functional.binary_cross_entropy_with_logits(x, ys)
		return output

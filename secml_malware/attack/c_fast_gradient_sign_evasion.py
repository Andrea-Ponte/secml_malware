import torch
import numpy as np
from secml.array import CArray
from secml.settings import SECML_PYTORCH_USE_CUDA
from secml_malware.attack import CEnd2EndMalwareEvasion
from secml_malware.models import CClassifierEnd2EndMalware

use_cuda = torch.cuda.is_available() and SECML_PYTORCH_USE_CUDA

class CFastGradientSignMethodEvasion(CEnd2EndMalwareEvasion):
	def __init__(
		self,
		end2end_model: CClassifierEnd2EndMalware,
		indexes_to_perturb: list,
		epsilon: float,
		iterations: int = 100,
		is_debug: bool = False,
		random_init: bool = False,
		threshold : float = 0.5,
		penalty_regularizer : float = 0,
		invalid_byte_value : int = -1,
		p_norm : float = np.infty
	):
		super(CFastGradientSignMethodEvasion, self).__init__(
			end2end_model=end2end_model,
			indexes_to_perturb=indexes_to_perturb,
			iterations=iterations,
			is_debug=is_debug,
			random_init=random_init,
			threshold=threshold,
			penalty_regularizer=penalty_regularizer,
			invalid_byte_value=invalid_byte_value
		)
		self.epsilon = epsilon
		self.p_norm = p_norm

	def compute_penalty_term(self, original_x: CArray, adv_x: CArray, par: float):
		penalty_term = torch.autograd.Variable([0])
		if use_cuda:
			penalty_term = penalty_term.cuda()
		return penalty_term

	def loss_function_gradient(self, original_x : CArray, adv_x : CArray, penalty_term : torch.Tensor):
		y = self.classifier.embedding_predict(adv_x)
		loss = torch.nn.BCELoss(y, 1)
		g = torch.autograd.grad(loss, adv_x)[0]
		g = torch.transpose(g, 1, 2)[0]
		return g

	def optimization_solver(self, E, gradient_f, index_to_consider, x_init):
		x_init[index_to_consider] = x_init[index_to_consider] + self._internal_fsgm_solver(gradient_f)
		return x_init

	def _internal_fsgm_solver(self, gradient_f):
		g = gradient_f / torch.norm(gradient_f) if not torch.equal(torch.zeros(gradient_f.shape), gradient_f) else torch.zeros(gradient_f.shape)
		if self.p_norm == 2:
			return  self.epsilon * g
		elif self.p_norm == np.infty:
			return self.epsilon * torch.sign(g)
		raise NotImplementedError(f"{self.p_norm}-norm not yet implemented")

	def infer_step(self, x_init):
		return self.classifier.embedding_predict(x_init)

	def invert_feature_mapping(self, x):
		E = self.get_embedded_byte_matrix()
		byte_malware = []
		if self.invalid_pos == -1:
			E = E[:256]
		else:
			E[0] = torch.tensor([np.infty for _ in range(self.classifier.get_embedding_size())])
		for i in range(x.shape[-1]):
			x_i = x[0, :, i]
			byte_to_consider = torch.tensor([torch.norm(e - x_i, p=2) for e in E]).argmin()
			byte_malware.append(byte_to_consider)
			if self.is_debug:
				print(f">{x_i} -> {byte_to_consider}")
		return CArray(x)

	def apply_feature_mapping(self, x : CArray):
		return self.classifier.embed(x.tondarray())
